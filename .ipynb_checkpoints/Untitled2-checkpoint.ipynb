{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now page is 0 and old data contains 0  duplicated observation.\n",
      "0\n",
      "Now page is 1 and old data contains 0  duplicated observation.\n",
      "Now page is 2 and old data contains 0  duplicated observation.\n",
      "Now page is 3 and old data contains 0  duplicated observation.\n",
      "Now page is 4 and old data contains 0  duplicated observation.\n",
      "Now page is 5 and old data contains 0  duplicated observation.\n",
      "5\n",
      "Now page is 6 and old data contains 0  duplicated observation.\n",
      "Now page is 7 and old data contains 0  duplicated observation.\n",
      "Now page is 8 and old data contains 0  duplicated observation.\n",
      "Now page is 9 and old data contains 0  duplicated observation.\n",
      "Now page is 10 and old data contains 0  duplicated observation.\n",
      "10\n",
      "Now page is 11 and old data contains 0  duplicated observation.\n",
      "Now page is 12 and old data contains 0  duplicated observation.\n",
      "Now page is 13 and old data contains 0  duplicated observation.\n",
      "Now page is 14 and old data contains 0  duplicated observation.\n",
      "Now page is 15 and old data contains 0  duplicated observation.\n",
      "15\n",
      "Now page is 16 and old data contains 0  duplicated observation.\n",
      "Now page is 17 and old data contains 0  duplicated observation.\n",
      "Now page is 18 and old data contains 0  duplicated observation.\n",
      "Now page is 19 and old data contains 0  duplicated observation.\n",
      "Now page is 20 and old data contains 0  duplicated observation.\n",
      "20\n",
      "Now page is 21 and old data contains 0  duplicated observation.\n",
      "Now page is 22 and old data contains 0  duplicated observation.\n",
      "Now page is 23 and old data contains 0  duplicated observation.\n",
      "Now page is 24 and old data contains 0  duplicated observation.\n",
      "Now page is 25 and old data contains 0  duplicated observation.\n",
      "25\n",
      "Now page is 26 and old data contains 0  duplicated observation.\n",
      "Now page is 27 and old data contains 0  duplicated observation.\n",
      "Now page is 28 and old data contains 0  duplicated observation.\n",
      "Now page is 29 and old data contains 0  duplicated observation.\n",
      "Now page is 30 and old data contains 0  duplicated observation.\n",
      "30\n",
      "Now page is 31 and old data contains 0  duplicated observation.\n",
      "Now page is 32 and old data contains 0  duplicated observation.\n",
      "Now page is 33 and old data contains 0  duplicated observation.\n",
      "Now page is 34 and old data contains 0  duplicated observation.\n",
      "Now page is 35 and old data contains 0  duplicated observation.\n",
      "35\n",
      "Now page is 36 and old data contains 0  duplicated observation.\n",
      "Now page is 37 and old data contains 0  duplicated observation.\n",
      "Now page is 38 and old data contains 0  duplicated observation.\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684574/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684588/\n",
      "Current page is 1\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684386/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1684465/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1684584/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1684589/\n",
      "Current page is 2\n",
      "Title can not find.Check url: https://tw.news.appledaily.com/forum/realtime/20191231/1684402/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684572/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684506/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684512/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684566/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684562/\n",
      "Current page is 3\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684561/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684553/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684552/\n",
      "Current page is 4\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684513/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/lifestyle/realtime/20191231/1683769/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1684487/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1684479/\n",
      "Current page is 5\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1684478/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684454/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1683479/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684445/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684392/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1684365/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684440/\n",
      "Current page is 6\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684436/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684421/\n",
      "Title can not find.Check url: https://tw.news.appledaily.com/forum/realtime/20191231/1684397/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1683048/\n",
      "Current page is 7\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/lifestyle/realtime/20191231/1682361/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/lifestyle/realtime/20191231/1682034/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684388/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684331/\n",
      "Current page is 8\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1684246/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/lifestyle/realtime/20191231/1684260/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/lifestyle/realtime/20191231/1684256/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/lifestyle/realtime/20191231/1684253/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1684346/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1684209/\n",
      "Current page is 9\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/lifestyle/realtime/20191231/1684352/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1684197/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1684191/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191231/1634801/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191231/1640044/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/lifestyle/realtime/20191231/1682235/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/lifestyle/realtime/20191231/1640046/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/lifestyle/realtime/20191231/1684284/\n",
      "Current page is 10\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191230/1684027/\n",
      "Title can not find.Check url: https://tw.news.appledaily.com/forum/realtime/20191230/1684309/\n",
      "Title can not find.Check url: https://tw.news.appledaily.com/forum/realtime/20191230/1684310/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191230/1684070/\n",
      "Title can not find.Check url: https://tw.news.appledaily.com/forum/realtime/20191230/1684313/\n",
      "Current page is 11\n",
      "Title can not find.Check url: https://tw.news.appledaily.com/forum/realtime/20191230/1684315/\n",
      "Title can not find.Check url: https://tw.news.appledaily.com/forum/realtime/20191230/1684317/\n",
      "Title can not find.Check url: https://tw.news.appledaily.com/forum/realtime/20191230/1684320/\n",
      "Title can not find.Check url: https://tw.news.appledaily.com/forum/realtime/20191230/1684323/\n",
      "Title can not find.Check url: https://tw.news.appledaily.com/forum/realtime/20191230/1684321/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/lifestyle/realtime/20191230/1682760/\n",
      "Current page is 12\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191230/1684291/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191230/1683890/\n",
      "Current page is 13\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191230/1684213/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191230/1684189/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191230/1684212/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191230/1683777/\n",
      "Current page is 14\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/animal/realtime/20191230/1684231/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191230/1684076/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191230/1684168/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191230/1684179/\n",
      "Current page is 15\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191230/1684004/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/gadget/realtime/20191230/1684186/\n",
      "Title can not find.Check url: https://tw.lifestyle.appledaily.com/animal/realtime/20191230/1684155/\n",
      "Title can not find.Check url: https://tw.sports.appledaily.com/realtime/20191230/1684042/\n",
      "Title can not find.Check url: https://tw.news.appledaily.com/forum/realtime/20191230/1684121/\n",
      "Current page is 16\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#抓出現在日期\n",
    "date_time=str(datetime.datetime.now()).split(' ')[0]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "path='/Users/ChingYunChuang/SENIOR_1/wed234python/final/log_py'+date_time+'.txt'\n",
    "#建立log，紀錄資訊\n",
    "if os.path.isfile(path):\n",
    "    logging.basicConfig(level=logging.WARNING,#控制檯列印的日誌級別\n",
    "                        filename=path,\n",
    "                        filemode='a') #模式有w和a，w就是寫模式，每次都會重新寫日誌，覆蓋之前的日誌;a是append的意思\n",
    "else:\n",
    "    logging.basicConfig(level=logging.WARNING,#控制檯列印的日誌級別\n",
    "                        filename=path,\n",
    "                        filemode='w') #模式有w和a，w就是寫模式，每次都會重新寫日誌，覆蓋之前的日誌;a是append的意思\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#讀取已有資料，找到最新資料時間\n",
    "D_old=pd.read_csv('/Users/ChingYunChuang/SENIOR_1/wed234python/final/apple_news_example.csv')\n",
    "\n",
    "\n",
    "# In[121]:\n",
    "\n",
    "\n",
    "#防爬機制 1\n",
    "user_agent = [\n",
    "\"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "\"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "\"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0\",\n",
    "\"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 3.0.30729; .NET CLR 3.5.30729; InfoPath.3; rv:11.0) like Gecko\",\n",
    "\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\",\n",
    "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "\"Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "\"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11\",\n",
    "\"Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11\",\n",
    "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\"\n",
    "]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#來抓目前及時能抓到的資料\n",
    "a=requests.get('https://tw.appledaily.com/new/realtime',headers={'User-Agent':np.random.choice(user_agent)})\n",
    "if a.status_code !=200:\n",
    "    print('Error QQ,can not access apple news')\n",
    "else:\n",
    "    soup=BeautifulSoup(a.text,'html.parser')\n",
    "    #soup = BeautifulSoup(html, \"lxml\")\n",
    "    page=1\n",
    "    #如果有下10頁，page=page+10\n",
    "    #如果沒有，找最後一頁是多少，讓page等於他\n",
    "    #蘋果不能直接抓到最大頁數，可惡QQ\n",
    "    while ('下10頁' in re.sub(string=soup.find(class_='page_switch').text,repl='',pattern='\\s+')):\n",
    "        page=page+10\n",
    "        a=requests.get('https://tw.appledaily.com/new/realtime/'+str(page),\n",
    "                       headers={'User-Agent':np.random.choice(user_agent)})\n",
    "        soup=BeautifulSoup(a.text,'html.parser')\n",
    "    temp=[p.get('title') for p in soup.find(class_='page_switch').find_all('a')]\n",
    "    page=int(temp[len(temp)-1])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#收集報導網址\n",
    "CATE=[]\n",
    "A_URL=[]\n",
    "\n",
    "#收集到舊資料X筆，代表開始重複，不繼續收集下去，以url為準(減少爬蟲時間與克服蘋果文章日期會改變的問題)\n",
    "restricted=15\n",
    "N=0\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "for i in range(0,page):\n",
    "    #如果超過限制，代表後來資料皆已收集過，不在收集，跳出迴圈\n",
    "    if N>restricted:\n",
    "        print('Now page is {i} and old data contains {N}duplicated observations, Stop!.'.format(i=i,N=N))\n",
    "        break\n",
    "    else:\n",
    "        print('Now page is {i} and old data contains {N}  duplicated observation.'.format(i=i,N=N))\n",
    "    #收集資料\n",
    "    a=requests.get('https://tw.appledaily.com/new/realtime/'+str(i+1),\n",
    "                   headers={'User-Agent':np.random.choice(user_agent)})\n",
    "    if a.status_code !=200:\n",
    "        print('Error QQ,can not access apple news')\n",
    "    else:\n",
    "        a.encoding='UTF-8'\n",
    "        soup=BeautifulSoup(a.text,'html.parser')\n",
    "        #soup = BeautifulSoup(html, 'html.parser')\n",
    "        #抓每個新聞分類，和網址\n",
    "        try:\n",
    "            target=soup.find_all('li',class_='rtddt')\n",
    "        except:\n",
    "            print('Can not find article information for page '+str(i+1))\n",
    "            continue\n",
    "        #save category\n",
    "        try:\n",
    "            category=[re.sub(string=s.find('h2').text,pattern='\\\\u3000|\\\\xa0',repl='') for s in target]\n",
    "        except:\n",
    "            print('Page '+str(i+1)+' is wrong for crawl category,please check')\n",
    "            category=[]\n",
    "        #save article url\n",
    "        try:\n",
    "            article_url=[s.find('a').get('href') for s in target]\n",
    "        except:\n",
    "            print('Page '+str(i+1)+' is wrong for crawl article url,please check')\n",
    "            article=[]\n",
    "        if len(article_url) != len(category):\n",
    "            raise RuntimeError('Big problem for unequal length for category & article_url')\n",
    "        else:\n",
    "            CATE.append(category)\n",
    "            A_URL.append(article_url)\n",
    "        #判斷現有資料是否已有此網址，紀錄文章次數，超過restricted即停止爬蟲\n",
    "        N+=len([s for s in D_old.url.isin(article_url) if s==True])\n",
    "    if i % 5 ==0:\n",
    "        print(i)\n",
    "        time.sleep(int(np.random.randint(1,10,1)))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#更新實際所需頁面\n",
    "page=i\n",
    "#資料格式\n",
    "D=pd.DataFrame(columns=['url','category','title','publish_time','content','additional'])\n",
    "#index\n",
    "n=0\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#抓取所有內容，轉成結構化資料\n",
    "for j in range(0,page):\n",
    "    cate=CATE[j]\n",
    "    target_url=A_URL[j]\n",
    "    if len(cate) != len(target_url):\n",
    "        raise RuntimeError('Big problem for length of category is not equal to article url')\n",
    "    else:\n",
    "        for i in range(0,len(cate)):\n",
    "            n=n+1\n",
    "            b=requests.get(target_url[i],\n",
    "                           headers={'User-Agent':np.random.choice(user_agent)})\n",
    "            if b.status_code !=200:\n",
    "                print('Problem for url: '+target_url[i])\n",
    "                continue\n",
    "            soup2=BeautifulSoup(b.text,'html.parser')\n",
    "            #抓標題\n",
    "            try:\n",
    "                title=re.sub(string=soup2.find('h1').text,pattern='\\\\u3000|\\\\xa0',repl='')\n",
    "            except:\n",
    "                print('Title can not find.Check url: '+target_url[i])\n",
    "                title=' '\n",
    "            #抓時間\n",
    "            try:\n",
    "                tt=soup2.find(class_='ndArticle_creat').text.split('：')[1]\n",
    "            except:\n",
    "                tt=' '\n",
    "            #抓內文\n",
    "            try:\n",
    "                txt=''\n",
    "                temp=soup2.find('div',class_='ndArticle_margin').select('p,div')\n",
    "                index1=[i for i,s in enumerate(temp) if (('改寫、轉貼分享，違者必究' in s.text) and i!=0)]\n",
    "                index1=index1[0]\n",
    "                temp=temp[0:index1:1]\n",
    "                #編按可用內容為0，所以不要抓取開頭為編按的文章\n",
    "                if re.search(string=temp[0].text,pattern='^編按：|^【編者按】') is not None:\n",
    "                    txt=' '\n",
    "                else:\n",
    "                    for s in temp:\n",
    "                        txt=txt+s.text\n",
    "                    #防止後來讀取產生NaN\n",
    "                    if txt == '':\n",
    "                        txt=' '\n",
    "                    else:\n",
    "                        txt=txt\n",
    "                    #如果有【即時論壇徵稿】後面為無意義雜訊，去除\n",
    "                    if '【即時論壇徵稿】' in txt:\n",
    "                        txt=txt[:(txt.find('【即時論壇徵稿】'))]\n",
    "                    else:\n",
    "                        txt=txt\n",
    "                    #去除空格、UTF-16編碼\n",
    "                    txt=txt.strip()\n",
    "                    txt=re.sub(string=txt,pattern='\\\\xa0|\\\\u3000',repl='')\n",
    "                    if '報導' in txt:\n",
    "                        txt=re.search(string=txt,pattern='.+（.+報導）|.+報導|.+\\(.+報導\\)').group(0)\n",
    "                    else:\n",
    "                        #雜訊混進資料\n",
    "                        if 'Frameborder，' in txt:\n",
    "                            txt=txt[:txt.find('Frameborder，')]\n",
    "                        else:\n",
    "                            txt=txt\n",
    "            except:\n",
    "                txt=' '\n",
    "            #抓額外圖片\n",
    "            add=''\n",
    "            #標題圖片\n",
    "            try:\n",
    "                additional=soup2.find(class_='ndAritcle_headPic').find('img').get('src')\n",
    "                add=add+additional+'，'\n",
    "            except:\n",
    "                add=add\n",
    "            #預防抓到很多不要的圖片，例如:編按內文藏了很多不該有的圖片\n",
    "            if txt == ' ':\n",
    "                if add=='':\n",
    "                    add=' '\n",
    "                else:\n",
    "                    add=re.sub(string=add,pattern='，',repl='')\n",
    "            else:\n",
    "                try:\n",
    "                    additional=soup2.find('div',class_='ndArticle_margin').find_all('img')\n",
    "                    additional=[s.get('src') for s in additional]\n",
    "                    if len(additional)>0:\n",
    "                        for k in range(0,len(additional)):\n",
    "                            if k != (len(additional)-1):\n",
    "                                add=add+additional[k]+'，'\n",
    "                            else:\n",
    "                                add=add+additional[k]\n",
    "                    else:\n",
    "                        add=re.sub(string=add,pattern='，',repl='')\n",
    "                except:\n",
    "                    add=re.sub(string=add,pattern='，',repl='')\n",
    "                #預防下次讀取NaN產生\n",
    "                if add == '':\n",
    "                    add=' '\n",
    "                else:\n",
    "                    add=add\n",
    "            #合併資料\n",
    "            dd={\n",
    "               'url':target_url[i],\n",
    "                'category':cate[i],\n",
    "                'title':title,\n",
    "                'publish_time':tt,\n",
    "                'content':txt,\n",
    "                'additional':add \n",
    "            }\n",
    "            d=pd.DataFrame(dd,columns=['url','category','title','publish_time','content','additional'],\n",
    "                           index=[n])\n",
    "            D=pd.concat([D,d],axis=0)\n",
    "            time.sleep(int(np.random.randint(1,8,1)))\n",
    "    print('Current page is '+str(j+1))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "D_temp=pd.concat([D,D_old],axis=0)\n",
    "D_temp=D_temp[~D_temp.duplicated(subset=['url','category'])]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#神奇編碼，如果僅存utf-8就會變亂碼....\n",
    "D_temp.to_csv('/Users/ChingYunChuang/Desktop/final/apple_news.csv',index=0,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
