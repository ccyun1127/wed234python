{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#抓出現在日期\n",
    "date_time=str(datetime.datetime.now()).split(' ')[0]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "path='/Users/ChingYunChuang/SENIOR_1/wed234python/final/log_py'+date_time+'.txt'\n",
    "#建立log，紀錄資訊\n",
    "if os.path.isfile(path):\n",
    "    logging.basicConfig(level=logging.WARNING,#控制檯列印的日誌級別\n",
    "                        filename=path,\n",
    "                        filemode='a') #模式有w和a，w就是寫模式，每次都會重新寫日誌，覆蓋之前的日誌;a是append的意思\n",
    "else:\n",
    "    logging.basicConfig(level=logging.WARNING,#控制檯列印的日誌級別\n",
    "                        filename=path,\n",
    "                        filemode='w') #模式有w和a，w就是寫模式，每次都會重新寫日誌，覆蓋之前的日誌;a是append的意思\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#讀取已有資料，找到最新資料時間\n",
    "D_old=pd.read_csv('/Users/ChingYunChuang/SENIOR_1/wed234python/final/apple_news_example.csv')\n",
    "\n",
    "\n",
    "# In[121]:\n",
    "\n",
    "\n",
    "#防爬機制 1\n",
    "user_agent = [\n",
    "\"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "\"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "\"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0\",\n",
    "\"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 3.0.30729; .NET CLR 3.5.30729; InfoPath.3; rv:11.0) like Gecko\",\n",
    "\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\",\n",
    "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "\"Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "\"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11\",\n",
    "\"Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11\",\n",
    "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\"\n",
    "]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#來抓目前及時能抓到的資料\n",
    "a=requests.get('https://tw.appledaily.com/new/realtime',headers={'User-Agent':np.random.choice(user_agent)})\n",
    "if a.status_code !=200:\n",
    "    print('Error QQ,can not access apple news')\n",
    "else:\n",
    "    soup=BeautifulSoup(a.text,'html.parser')\n",
    "    #soup = BeautifulSoup(html, \"lxml\")\n",
    "    page=1\n",
    "    #如果有下10頁，page=page+10\n",
    "    #如果沒有，找最後一頁是多少，讓page等於他\n",
    "    #蘋果不能直接抓到最大頁數，可惡QQ\n",
    "    while ('下10頁' in re.sub(string=soup.find(class_='page_switch').text,repl='',pattern='\\s+')):\n",
    "        page=page+10\n",
    "        a=requests.get('https://tw.appledaily.com/new/realtime/'+str(page),\n",
    "                       headers={'User-Agent':np.random.choice(user_agent)})\n",
    "        soup=BeautifulSoup(a.text,'html.parser')\n",
    "    temp=[p.get('title') for p in soup.find(class_='page_switch').find_all('a')]\n",
    "    page=int(temp[len(temp)-1])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#收集報導網址\n",
    "CATE=[]\n",
    "A_URL=[]\n",
    "\n",
    "#收集到舊資料X筆，代表開始重複，不繼續收集下去，以url為準(減少爬蟲時間與克服蘋果文章日期會改變的問題)\n",
    "restricted=15\n",
    "N=0\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "for i in range(0,page):\n",
    "    #如果超過限制，代表後來資料皆已收集過，不在收集，跳出迴圈\n",
    "    if N>restricted:\n",
    "        print('Now page is {i} and old data contains {N}duplicated observations, Stop!.'.format(i=i,N=N))\n",
    "        break\n",
    "    else:\n",
    "        print('Now page is {i} and old data contains {N}  duplicated observation.'.format(i=i,N=N))\n",
    "    #收集資料\n",
    "    a=requests.get('https://tw.appledaily.com/new/realtime/'+str(i+1),\n",
    "                   headers={'User-Agent':np.random.choice(user_agent)})\n",
    "    if a.status_code !=200:\n",
    "        print('Error QQ,can not access apple news')\n",
    "    else:\n",
    "        a.encoding='UTF-8'\n",
    "        soup=BeautifulSoup(a.text,'html.parser')\n",
    "        #soup = BeautifulSoup(html, 'html.parser')\n",
    "        #抓每個新聞分類，和網址\n",
    "        try:\n",
    "            target=soup.find_all('li',class_='rtddt')\n",
    "        except:\n",
    "            print('Can not find article information for page '+str(i+1))\n",
    "            continue\n",
    "        #save category\n",
    "        try:\n",
    "            category=[re.sub(string=s.find('h2').text,pattern='\\\\u3000|\\\\xa0',repl='') for s in target]\n",
    "        except:\n",
    "            print('Page '+str(i+1)+' is wrong for crawl category,please check')\n",
    "            category=[]\n",
    "        #save article url\n",
    "        try:\n",
    "            article_url=[s.find('a').get('href') for s in target]\n",
    "        except:\n",
    "            print('Page '+str(i+1)+' is wrong for crawl article url,please check')\n",
    "            article=[]\n",
    "        if len(article_url) != len(category):\n",
    "            raise RuntimeError('Big problem for unequal length for category & article_url')\n",
    "        else:\n",
    "            CATE.append(category)\n",
    "            A_URL.append(article_url)\n",
    "        #判斷現有資料是否已有此網址，紀錄文章次數，超過restricted即停止爬蟲\n",
    "        N+=len([s for s in D_old.url.isin(article_url) if s==True])\n",
    "    if i % 5 ==0:\n",
    "        print(i)\n",
    "        time.sleep(int(np.random.randint(1,10,1)))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#更新實際所需頁面\n",
    "page=i\n",
    "#資料格式\n",
    "D=pd.DataFrame(columns=['url','category','title','publish_time','content','additional'])\n",
    "#index\n",
    "n=0\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#抓取所有內容，轉成結構化資料\n",
    "for j in range(0,page):\n",
    "    cate=CATE[j]\n",
    "    target_url=A_URL[j]\n",
    "    if len(cate) != len(target_url):\n",
    "        raise RuntimeError('Big problem for length of category is not equal to article url')\n",
    "    else:\n",
    "        for i in range(0,len(cate)):\n",
    "            n=n+1\n",
    "            b=requests.get(target_url[i],\n",
    "                           headers={'User-Agent':np.random.choice(user_agent)})\n",
    "            if b.status_code !=200:\n",
    "                print('Problem for url: '+target_url[i])\n",
    "                continue\n",
    "            soup2=BeautifulSoup(b.text,'html.parser')\n",
    "            #抓標題\n",
    "            try:\n",
    "                title=re.sub(string=soup2.find('h1').text,pattern='\\\\u3000|\\\\xa0',repl='')\n",
    "            except:\n",
    "                print('Title can not find.Check url: '+target_url[i])\n",
    "                title=' '\n",
    "            #抓時間\n",
    "            try:\n",
    "                tt=soup2.find(class_='ndArticle_creat').text.split('：')[1]\n",
    "            except:\n",
    "                tt=' '\n",
    "            #抓內文\n",
    "            try:\n",
    "                txt=''\n",
    "                temp=soup2.find('div',class_='ndArticle_margin').select('p,div')\n",
    "                index1=[i for i,s in enumerate(temp) if (('改寫、轉貼分享，違者必究' in s.text) and i!=0)]\n",
    "                index1=index1[0]\n",
    "                temp=temp[0:index1:1]\n",
    "                #編按可用內容為0，所以不要抓取開頭為編按的文章\n",
    "                if re.search(string=temp[0].text,pattern='^編按：|^【編者按】') is not None:\n",
    "                    txt=' '\n",
    "                else:\n",
    "                    for s in temp:\n",
    "                        txt=txt+s.text\n",
    "                    #防止後來讀取產生NaN\n",
    "                    if txt == '':\n",
    "                        txt=' '\n",
    "                    else:\n",
    "                        txt=txt\n",
    "                    #如果有【即時論壇徵稿】後面為無意義雜訊，去除\n",
    "                    if '【即時論壇徵稿】' in txt:\n",
    "                        txt=txt[:(txt.find('【即時論壇徵稿】'))]\n",
    "                    else:\n",
    "                        txt=txt\n",
    "                    #去除空格、UTF-16編碼\n",
    "                    txt=txt.strip()\n",
    "                    txt=re.sub(string=txt,pattern='\\\\xa0|\\\\u3000',repl='')\n",
    "                    if '報導' in txt:\n",
    "                        txt=re.search(string=txt,pattern='.+（.+報導）|.+報導|.+\\(.+報導\\)').group(0)\n",
    "                    else:\n",
    "                        #雜訊混進資料\n",
    "                        if 'Frameborder，' in txt:\n",
    "                            txt=txt[:txt.find('Frameborder，')]\n",
    "                        else:\n",
    "                            txt=txt\n",
    "            except:\n",
    "                txt=' '\n",
    "            #抓額外圖片\n",
    "            add=''\n",
    "            #標題圖片\n",
    "            try:\n",
    "                additional=soup2.find(class_='ndAritcle_headPic').find('img').get('src')\n",
    "                add=add+additional+'，'\n",
    "            except:\n",
    "                add=add\n",
    "            #預防抓到很多不要的圖片，例如:編按內文藏了很多不該有的圖片\n",
    "            if txt == ' ':\n",
    "                if add=='':\n",
    "                    add=' '\n",
    "                else:\n",
    "                    add=re.sub(string=add,pattern='，',repl='')\n",
    "            else:\n",
    "                try:\n",
    "                    additional=soup2.find('div',class_='ndArticle_margin').find_all('img')\n",
    "                    additional=[s.get('src') for s in additional]\n",
    "                    if len(additional)>0:\n",
    "                        for k in range(0,len(additional)):\n",
    "                            if k != (len(additional)-1):\n",
    "                                add=add+additional[k]+'，'\n",
    "                            else:\n",
    "                                add=add+additional[k]\n",
    "                    else:\n",
    "                        add=re.sub(string=add,pattern='，',repl='')\n",
    "                except:\n",
    "                    add=re.sub(string=add,pattern='，',repl='')\n",
    "                #預防下次讀取NaN產生\n",
    "                if add == '':\n",
    "                    add=' '\n",
    "                else:\n",
    "                    add=add\n",
    "            #合併資料\n",
    "            dd={\n",
    "               'url':target_url[i],\n",
    "                'category':cate[i],\n",
    "                'title':title,\n",
    "                'publish_time':tt,\n",
    "                'content':txt,\n",
    "                'additional':add \n",
    "            }\n",
    "            d=pd.DataFrame(dd,columns=['url','category','title','publish_time','content','additional'],\n",
    "                           index=[n])\n",
    "            D=pd.concat([D,d],axis=0)\n",
    "            time.sleep(int(np.random.randint(1,8,1)))\n",
    "    print('Current page is '+str(j+1))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "D_temp=pd.concat([D,D_old],axis=0)\n",
    "D_temp=D_temp[~D_temp.duplicated(subset=['url','category'])]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#神奇編碼，如果僅存utf-8就會變亂碼....\n",
    "D_temp.to_csv('/Users/ChingYunChuang/Desktop/final/apple_news.csv',index=0,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
